{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Stack Exchance\n",
    "\n",
    "While everyone *loves* a fun dataset to explore, good data is expensive. It costs a significant amount of resources to generate, accurately curate, securely store, and provide robust access to. For instance, our cold-storage tape archive, [Ranch](https://www.tacc.utexas.edu/systems/ranch), grows at a rate of 8.5PB (~5.3%) per year. Despite these costs, data is often invaluable to both users and administrators.\n",
    "\n",
    "Today, we will be exploring data from Stack Exchange. While this is probably not the kind of data you interact with on a daily basis, everyone at this camp should have some familiarity from interacting with *at least* one [Stack Exchange Community](https://stackexchange.com/sites):\n",
    "\n",
    "- StackOverflow\n",
    "- Super User\n",
    "- TeX - LaTeX\n",
    "- ...and more\n",
    "\n",
    "Today, you will be using python to explore question and answer history from the Stack Exchange site of your choice. This data will be accessed over their public API. This is their **actual** data, and these methods can be extended to a variety of other datasets and websites.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Use python [requests](http://docs.python-requests.org/en/master/) to download data\n",
    "- Import data into [Pandas](http://pandas.pydata.org/)\n",
    "- Explore data\n",
    "  - Inspect and summarize data\n",
    "  - Group records\n",
    "  - Select and subset records\n",
    "  - Visualize selection\n",
    "  - Join two datasets together\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "We will be using the following non-standard python libraries:\n",
    "- [**requests** library](http://docs.python-requests.org/en/master/) *\\(Already Installed\\)*\n",
    "- [**pandas** library](http://pandas.pydata.org/) *\\(Already Installed\\)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "# Render matplotlib in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Exchange Questions\n",
    "\n",
    "Stack Exchange has a [well documented API](https://api.stackexchange.com/), which contains enpoints for **each site**. You can perform any graphical interaction through the API while authenticated, but general information can also be retrieved anonymously. Just make sure you do not make more than 10,000 requests per day. (*I did while devloping this notebook*)\n",
    "\n",
    "Beginning with the initial questsions submitted by users, take a look at the\n",
    "\n",
    "[Questions API](https://api.stackexchange.com/docs/questions)\n",
    "\n",
    "webapp on the Stack Exchange site, and build a query that you would like to use with Python.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Choose a site (default is StackOverflow)\n",
    "- Choose Start and/or End Date\n",
    "- Sort by creation\n",
    "- Limit the number of questions to 10 (`pagesize`)\n",
    "\n",
    "# Make API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API URL\n",
    "url = 'https://api.stackexchange.com/2.2/questions'\n",
    "\n",
    "params = dict(\n",
    "    site='stackoverflow', # stackoverflow (coding) questions\n",
    "    pagesize='10',        # Number of questions to return\n",
    "    fromdate='1500163200',# Get epoch time from webapp\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "resp = requests.get(url=url, params=params)\n",
    "data = json.loads(resp.text)\n",
    "\n",
    "print(json.dumps(data, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! If you kept `pagesize` at 10, you should have a JSON response of 10 questions. If you decided to crank up your response size, you might have to scroll a bit.\n",
    "\n",
    "## JSON Structure\n",
    "\n",
    "This JSON response probably looks familiar if you have ever worked with Python dictionaries in the past. At the most basic level, a JSON is a collection of key and value pairs.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key1\": value1,\n",
    "    \"key2\": value2\n",
    "}\n",
    "```\n",
    "\n",
    "Instead of using a numerical index, you refer to each value with the corresponding key.\n",
    "\n",
    "- key1\n",
    "- key2\n",
    "\n",
    "This makes both the data structure and programatic access human-readable. However, the lack of indicies makes traditional access through looping somewhat difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first question title\n",
    "############################\n",
    "# Pull \"items\" json\n",
    "#  > Pull first record\n",
    "#    > Pull title\n",
    "print(data['items'][0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to know the exact key names to traverse it\n",
    "for item in data['items']:\n",
    "    # Print the question title\n",
    "    print(\"TAGS - %s\"%(item['title']))\n",
    "    # Print the question tags\n",
    "    print(\"   [%s]\\n\"%(\", \".join(item['tags'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "- Try pulling out the `answer_count` for each question\n",
    "- Try pulling out the `view_count` for each question\n",
    "- Try pulling out the submission date.\n",
    "- **Extra Credit** - [Convert the epoch time to human readable](https://stackoverflow.com/a/12400584)\n",
    "\n",
    "# Converting to Pandas\n",
    "\n",
    "Instead of testing you on your ability to traverse a JSON tree, the goal for today is to explore data using Pandas, so lets convert the JSON to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questionsDF = pd.io.json.json_normalize(data['items'])\n",
    "questionsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`json_normalize`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) takes a nested JSON and flattens it into a table. In our case, it flattened each return question in the `items` list. Child JSONs like owner, which described the original submitter, now have owner as a prefix in the column name.\n",
    "\n",
    "### JSON\n",
    "```\n",
    "\"owner\": {\n",
    "            \"reputation\": 1,\n",
    "            \"user_id\": 6140730,\n",
    "            \"user_type\": \"registered\",\n",
    "            \"profile_image\": \"https://www.gravatar.com/avatar/efa02138df0bc1f59618c365872caed6?s=128&d=identicon&r=PG&f=1\",\n",
    "            \"display_name\": \"John\",\n",
    "            \"link\": \"https://stackoverflow.com/users/6140730/john\"\n",
    "         }\n",
    "```\n",
    "### Table\n",
    "\n",
    "| Column Name | Value |\n",
    "|--|--|\n",
    "| owner.reputation | 1 |\n",
    "| owner.user_id | 6140730 |\n",
    "| owner.user_type | registered |\n",
    "| owner.profile_image | https://www... |\n",
    "| owner.display_name | John |\n",
    "| owner.link | https://stackoverflow... |\n",
    "\n",
    "# Exploring the Data\n",
    "\n",
    "When we transform the JSON data into a table, using `json_normalize`, the resulting table is actually a [Pandas DataFrame.](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "\n",
    "A DataFrame is a 2-dimensional data structure that can store data of different types\n",
    "\n",
    "(characters, integers, floating point values, factors, and more)\n",
    "\n",
    "in columns. It is similar to a spreadsheet or an SQL table or the data.frame in R. A DataFrame always has an index (0-based). An index refers to the row of an element in the data structure.\n",
    "\n",
    "You can see the **bold** index column on the left of our example.\n",
    "\n",
    "## Viewing DataFrame Attributes\n",
    "\n",
    "Besides having text column headers, DataFrames come with some nice attributes and methods to view specific parts of the data.\n",
    "\n",
    "## Columns\n",
    "\n",
    "You often need to iterate over the columns of your table, and DataFrames expose those names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questionsDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape\n",
    "\n",
    "You can also see how many rows and columns (rows, columns) are in your DataFrame by accessing the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questionsDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head\n",
    "\n",
    "If you have ever used the `head` command on a terminal to view the first N lines of a file, the head function of a DataFrame will look familiar to you. This is great for just peeking at the data and not overflowing your window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.head()\n",
    "#questionsDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tail\n",
    "\n",
    "There is also a tail command for looking at the last N rows of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.tail()\n",
    "#questionsDF.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Records\n",
    "\n",
    "Many of the columns in this data, like `owner.link`, may not be immediately useful to us. With a DataFrame, you can select and group specific columns for use in a downstream analysis without losing the original.\n",
    "\n",
    "For example, we could be interested in the `view_count` of each question. An analysis of this column could show how many people also encounter a similar problem and needed to seek help on Stack Exchange.\n",
    "\n",
    "## Column Groups\n",
    "\n",
    "We can pull out this single column using two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dot\n",
    "print(questionsDF.view_count.head())\n",
    "# Bracket\n",
    "print(questionsDF['view_count'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce similar statistics provided by the `summary()` function in R with the `describe()` function. This can be applied directly to our column selection as so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides only using the first 10 questions in my example data, they're all very new, so they have very few views. Lets instead work on the latest 1,000 questions and generate the same description. Stack Exchange [limits](https://api.stackexchange.com/docs/throttle) the `pagesize` of the response to 100, so we will be pulling the first 10 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest 1000 questions\n",
    "\n",
    "# Params pull 100 questsions per query\n",
    "params = dict(\n",
    "    site='stackoverflow',\n",
    "    pagesize='100',\n",
    "    page='1',\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "nPages = 10 #How many pages you want\n",
    "data = []\n",
    "\n",
    "import sys\n",
    "print(\"Reading Page:\")\n",
    "for page in map(str, range(1,nPages+1)):\n",
    "    params['page']=page # Change page number\n",
    "    if int(page) > 1: sys.stdout.write(\", \")\n",
    "    sys.stdout.write(\"%s\"%(page))\n",
    "    data += json.loads(requests.get(url=url, params=params).text)['items']\n",
    "\n",
    "questionsDF = pd.io.json.json_normalize(data)\n",
    "# Drop the \"migrated_from\" columns\n",
    "questionsDF = questionsDF[list(filter(lambda x: 'migrated' not in x, questionsDF.columns))]\n",
    "\n",
    "questionsDF['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a larger pool of data, you should check out other statistics that can be generated per column. Feel free to use another numerical column as well.\n",
    "\n",
    "## Explore\n",
    "\n",
    "There are a bunch of [built in](https://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats) descriptive functions, but these are good to check out.\n",
    "\n",
    "- describe()\n",
    "- nuniqe()\n",
    "- value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique users?\n",
    "questionsDF['owner.user_id'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Way Groups\n",
    "\n",
    "If you ever want to summarize by one or more variables, you can use the `groupby` method. In our case, it would be interesting to look at `view_count` statistics of answered and unanswered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.groupby('is_answered')['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while there are fewer answered questions, their view count (in my test) is almost 100% higher. Neat!\n",
    "\n",
    "## Explore\n",
    "\n",
    "Take some time using the `groupby` method to explore other cool trends.\n",
    "\n",
    "- Owner reputation - Is the submitter a bot?\n",
    "- Score - Is the question real?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questionsDF.groupby('is_answered')['owner.reputation'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Subsetting Records\n",
    "\n",
    "You can also select a subset of the data using criteria. For example, we can select all rows that have a `view_count` greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF[questionsDF.view_count > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "Experiment with the\n",
    "\n",
    "- `>`, `<`\n",
    "- `==`, `!=`\n",
    "- `>=`, `<=`\n",
    "\n",
    "operators on numerical data. If you have extra time, look for questions that contain tags that you know. The tags are actually a list, so you can search for tags using the `in` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use the map operation on tags\n",
    "questionsDF[questionsDF.tags.map(lambda x: 'python' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Results\n",
    "\n",
    "While the tables we have been generating are nice, they still contain thousands of rows. A single figure could help visualize the data as a whole. Insead of crafting specific matplotlib calls, Pandas built a universal [`plot()` function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) into the DataFrame object to simplify figure generation.\n",
    "\n",
    "By stating that we want to generate a histogram with `kind='hist'`, we can look at the `view_count` fequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF['view_count'].plot(kind='hist')\n",
    "# Try increasing the resolution with the \"bins\" parameter\n",
    "# Try a square root transform of the view count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot our two-way tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.groupby('is_answered')['view_count'].plot(kind='hist', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "Try generating a few figures on your own.\n",
    "\n",
    "# Joining Tables\n",
    "\n",
    "You can even join two datasets. Lets grab some answers so we can try joining them to their corresponding questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.stackexchange.com/2.2/answers'\n",
    "params = dict(\n",
    "    site='stackoverflow',\n",
    "    pagesize='100',\n",
    "    page='1',\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "nPages = 10 #How many pages you want\n",
    "data = []\n",
    "\n",
    "import sys\n",
    "print(\"Reading Page:\")\n",
    "for page in map(str, range(1,nPages+1)):\n",
    "    params['page']=page # Change page number\n",
    "    if int(page) > 1: sys.stdout.write(\", \")\n",
    "    sys.stdout.write(\"%s\"%(page))\n",
    "    data += json.loads(requests.get(url=url, params=params).text)['items']\n",
    "\n",
    "answersDF = pd.io.json.json_normalize(data)\n",
    "answersDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "\n",
    "We can return the intersection of all questions that also map to an answer by using an inner join. Assuming we had the following example data:\n",
    "\n",
    "```\n",
    "Questions\n",
    "---------------------\n",
    "QuestionID 0  1  2  3\n",
    "ViewCount  2  4  10 7\n",
    "AnswerID   NA 1  2  3\n",
    "\n",
    "Answers\n",
    "---------------------\n",
    "QuestionID 5  1  2  3\n",
    "Score      3  5  3  1\n",
    "AnswerID   0  1  2  3\n",
    "```\n",
    "\n",
    "An inner join would yield\n",
    "\n",
    "```\n",
    "Questions X Answers\n",
    "---------------------\n",
    "QuestionID 1  2  3\n",
    "ViewCount  4  10 7\n",
    "Score      5  3  1\n",
    "AnswerID   1  2  3\n",
    "```\n",
    "\n",
    "We join both `questionsDF` and `answersDF` on the `question_id` column that they both share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=questionsDF, right=answersDF[['answer_id','question_id']], left_on=\"question_id\", right_on=\"question_id\")\n",
    "print(merged.shape)\n",
    "merged.head()\n",
    "print(questionsDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Join\n",
    "\n",
    "Left joins return all items from the first set, and any items from the second set that overlap with the first. This is useful if we want ALL questions returned, and any questions that also match.\n",
    "\n",
    "Using the table from the first example, a left join would yield\n",
    "\n",
    "```\n",
    "Questions LJ Answers\n",
    "---------------------\n",
    "QuestionID 0  1  2  3\n",
    "ViewCount  2  4  10 7\n",
    "Score      NA 5  3  1\n",
    "AnswerID   NA 1  2  3\n",
    "```\n",
    "\n",
    "Notice that whenever there is no match on the right, fields are filled in as NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=questionsDF, right=answersDF, left_on=\"question_id\", right_on=\"question_id\", how=\"left\")\n",
    "print(merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "There are also Right and Outer joins to explore. Take a look at [the documentation](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) and see if you can discover anythign fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try joining some data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
